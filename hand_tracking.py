import cv2
import mediapipe as mp
import numpy as np
# import tensorflow as tf
import tflite_runtime.interpreter as tflite


SCALING_FACTOR = 1.5  # í™”ë©´ í¬ê¸° ì¡°ì • ë¹„ìœ¨

# âœ… ì›¹ìº ì„ ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  íŒŒì¼ì—ì„œ ê³µìœ 
cap = cv2.VideoCapture(0)

# âœ… MediaPipe ì† ì¸ì‹ ì´ˆê¸°í™”
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7, min_tracking_confidence=0.7)

# âœ… ì†ë™ì‘ ì¸ì‹ (ì œìŠ¤ì²˜ ë§¤í•‘)
rps_gesture = {
    0: 'zero',
    1: 'one',
    2: 'two',  # pen/erase
    3: 'three',
    4: 'four',
    5: 'five',  # toggle
    6: 'scissors',
    7: 'spiderman',
    8: 'okay',
}

# âœ… TFLite ëª¨ë¸ ë¡œë“œ
model_name = 'hand_keypoint_classifier_weights[leaky_3dense].weights'
interpreter = tflite.Interpreter(model_path=f'weights/{model_name}.tflite')
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# âœ… ì† ì¸ì‹ í•¨ìˆ˜ (ì›¹ìº  ì‚¬ìš©)
def detect_hand_gesture(screen_width, screen_height):
    """ì† ì¸ì‹ í›„ ì†ë™ì‘ê³¼ ì¢Œí‘œ ë°˜í™˜"""
    
    # ğŸ”¹ ì›¹ìº ì´ ì—´ë ¤ ìˆëŠ”ì§€ í™•ì¸
    if not cap.isOpened():
        print("[ERROR] ì›¹ìº ì´ ì—´ë ¤ ìˆì§€ ì•ŠìŒ")
        return "None", screen_width // 2, screen_height // 2
    
    ret, frame = cap.read()
    if not ret:
        print("[ERROR] ì›¹ìº ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŒ")
        return "None", screen_width // 2, screen_height // 2

    frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = hands.process(rgb_frame)

    cursor_x, cursor_y = None, None
    hand_data = "None"

    if result.multi_hand_landmarks:
        for res in result.multi_hand_landmarks:
            joint = np.zeros((21, 3))
            for j, lm in enumerate(res.landmark):
                joint[j] = [lm.x, lm.y, lm.z]

            # ë²¡í„° ê³„ì‚°
            v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19], :]
            v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], :]
            v = v2 - v1

            if np.linalg.norm(v, axis=1).min() == 0:
                continue  # ì˜ˆì™¸ ì²˜ë¦¬

            v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]

            angle = np.arccos(np.einsum('nt,nt->n',
                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],
                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]))
            angle = np.degrees(angle)
            data = np.array([angle], dtype=np.float32)

            # âœ… TFLite ëª¨ë¸ì„ ì´ìš©í•œ ì œìŠ¤ì²˜ ì¸ì‹
            interpreter.set_tensor(input_details[0]['index'], data)
            interpreter.invoke()
            output_data = interpreter.get_tensor(output_details[0]['index'])
            predicted_idx = np.argmax(output_data)
            
            hand_data = rps_gesture[predicted_idx] if predicted_idx in rps_gesture else "None"

            # ì†ì˜ ì¢Œí‘œ ê°€ì ¸ì˜¤ê¸° (ê²€ì§€ì†ê°€ë½ ê¸°ì¤€)
            raw_x = res.landmark[12].x * screen_width
            raw_y = res.landmark[12].y * screen_height

            # ì¤‘ì•™ ê¸°ì¤€ ìƒëŒ€ì  ê±°ë¦¬ 1.5ë°° í™•ëŒ€
            cursor_x = int((raw_x - screen_width // 2) * SCALING_FACTOR + screen_width // 2)
            cursor_y = int((raw_y - screen_height // 2) * SCALING_FACTOR + screen_height // 2) + 120

            # í™”ë©´ì„ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì œí•œ
            cursor_x = min(max(0, cursor_x), screen_width - 1)
            cursor_y = min(max(0, cursor_y), screen_height - 1)


    # ì†ì´ ê°ì§€ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ì¢Œí‘œ ì„¤ì •
    if cursor_x is None or cursor_y is None:
        cursor_x, cursor_y = screen_width // 2, screen_height // 2

    # print(f"[hand_tracking.py] hand_data: {hand_data}, cursor_x: {cursor_x}, cursor_y: {cursor_y}")
    return hand_data, cursor_x, cursor_y
 